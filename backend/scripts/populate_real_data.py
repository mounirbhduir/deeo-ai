#!/usr/bin/env python3
"""
Script de peuplement automatique de la base STAGING avec donn√©es r√©elles.

Usage:
    python scripts/populate_real_data.py [--max-publications 15000] [--batch-size 100]

√âtapes:
    1. Collecte publications arXiv (cat√©gories IA)
    2. Classification ML th√©matique (BART zero-shot)
    3. V√©rification coh√©rence donn√©es

Note: L'enrichissement Semantic Scholar sera ajout√© dans une version ult√©rieure.
"""

import asyncio
import argparse
import sys
from datetime import datetime, timedelta
from typing import List, Dict, Any
from pathlib import Path

# Add backend to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from app.database import AsyncSessionLocal
from app.pipelines.arxiv_pipeline import ArxivPipeline, ArxivPipelineStats
from app.pipelines.ml_classifier import MLClassifierService
from app.repositories.publication_repository import PublicationRepository
from app.repositories.auteur_repository import AuteurRepository
from app.repositories.theme_repository import ThemeRepository
from app.logging import get_logger

logger = get_logger(__name__)


class RealDataPopulator:
    """Orchestrateur de peuplement donn√©es r√©elles depuis arXiv."""

    # Cat√©gories arXiv √† collecter
    AI_CATEGORIES = [
        'cs.AI',   # Artificial Intelligence
        'cs.LG',   # Machine Learning
        'cs.CV',   # Computer Vision
        'cs.CL',   # Computation and Language
        'cs.NE',   # Neural and Evolutionary Computing
        'stat.ML'  # Machine Learning (Statistics)
    ]

    # Requ√™tes de recherche pour diversifier les publications
    SEARCH_QUERIES = [
        "deep learning",
        "neural networks",
        "transformer",
        "reinforcement learning",
        "computer vision",
        "natural language processing",
        "machine learning",
        "artificial intelligence",
    ]

    def __init__(
        self,
        max_publications: int = 15000,
        batch_size: int = 100,
        date_range_months: int = 24
    ):
        """
        Initialize populator.

        Args:
            max_publications: Nombre maximum de publications √† collecter
            batch_size: Taille des batchs pour les requ√™tes arXiv
            date_range_months: Nombre de mois en arri√®re pour la collecte
        """
        self.max_publications = max_publications
        self.batch_size = batch_size
        self.date_range_months = date_range_months

        # Calculate date range
        self.end_date = datetime.now()
        self.start_date = self.end_date - timedelta(days=date_range_months * 30)

        # Statistics
        self.stats = {
            'total_collected': 0,
            'total_created': 0,
            'total_updated': 0,
            'total_skipped': 0,
            'total_authors': 0,
            'total_themes': 0,
            'total_errors': 0,
            'category_stats': {},
            'query_stats': {},
        }

    async def run(self):
        """Ex√©cuter pipeline complet de peuplement."""
        logger.info("=" * 80)
        logger.info("üöÄ D√âMARRAGE PEUPLEMENT STAGING AVEC DONN√âES R√âELLES")
        logger.info("=" * 80)
        logger.info(f"Configuration:")
        logger.info(f"  - Max publications: {self.max_publications}")
        logger.info(f"  - Batch size: {self.batch_size}")
        logger.info(f"  - Date range: {self.start_date.date()} to {self.end_date.date()}")
        logger.info(f"  - Categories: {', '.join(self.AI_CATEGORIES)}")
        logger.info("")

        start_time = datetime.now()

        try:
            # √âtape 1 : Collecte arXiv avec ETL pipeline
            await self._collect_arxiv_publications()

            # √âtape 2 : Classification ML (si n√©cessaire)
            await self._classify_publications()

            # √âtape 3 : Statistiques finales
            await self._display_final_stats()

        except KeyboardInterrupt:
            logger.warning("\n‚ö†Ô∏è  Interruption utilisateur d√©tect√©e")
            self.stats['errors'] = self.stats.get('errors', 0) + 1
        except Exception as e:
            logger.error(f"‚ùå Erreur fatale : {e}", exc_info=True)
            self.stats['errors'] = self.stats.get('errors', 0) + 1
        finally:
            duration = datetime.now() - start_time
            logger.info("")
            logger.info("=" * 80)
            logger.info(f"‚úÖ PEUPLEMENT TERMIN√â EN {duration}")
            logger.info("=" * 80)

    async def _collect_arxiv_publications(self):
        """√âtape 1 : Collecter publications depuis arXiv avec pipeline ETL."""
        logger.info("\n" + "=" * 80)
        logger.info("üì• √âTAPE 1/2 : COLLECTE PUBLICATIONS ARXIV")
        logger.info("=" * 80)

        publications_per_query = self.max_publications // len(self.SEARCH_QUERIES)

        async with AsyncSessionLocal() as session:
            async with ArxivPipeline(session) as pipeline:
                for idx, query in enumerate(self.SEARCH_QUERIES, 1):
                    logger.info(f"\n[{idx}/{len(self.SEARCH_QUERIES)}] üîç Requ√™te: '{query}'")

                    try:
                        # Run pipeline for this query
                        stats = await pipeline.run(
                            query=query,
                            categories=self.AI_CATEGORIES,
                            date_range=(self.start_date, self.end_date),
                            max_results=publications_per_query,
                        )

                        # Update global stats
                        self._update_stats_from_pipeline(query, stats)

                        # Display progress
                        logger.info(f"  ‚úÖ Collect√©es: {stats.papers_collected}")
                        logger.info(f"  ‚ûï Cr√©√©es: {stats.papers_created}")
                        logger.info(f"  üîÑ Mises √† jour: {stats.papers_updated}")
                        logger.info(f"  ‚è≠Ô∏è  Ignor√©es: {stats.papers_skipped}")
                        logger.info(f"  üë• Auteurs: {stats.authors_created}")
                        logger.info(f"  üè∑Ô∏è  Th√®mes: {stats.themes_created}")

                        if stats.errors > 0:
                            logger.warning(f"  ‚ö†Ô∏è  Erreurs: {stats.errors}")

                        # Check if we reached max publications
                        if self.stats['total_created'] >= self.max_publications:
                            logger.info(f"\n‚úÖ Objectif atteint : {self.stats['total_created']} publications")
                            break

                    except Exception as e:
                        logger.error(f"‚ùå Erreur requ√™te '{query}': {e}")
                        self.stats['total_errors'] += 1
                        continue

        logger.info(f"\nüìä Total publications collect√©es : {self.stats['total_collected']}")
        logger.info(f"üìä Total publications cr√©√©es : {self.stats['total_created']}")

    async def _classify_publications(self):
        """√âtape 2 : Classification ML th√©matique (optionnelle)."""
        logger.info("\n" + "=" * 80)
        logger.info("ü§ñ √âTAPE 2/2 : CLASSIFICATION ML TH√âMATIQUE")
        logger.info("=" * 80)

        async with AsyncSessionLocal() as session:
            pub_repo = PublicationRepository(session)
            classifier = MLClassifierService(session)

            # Get publications without theme
            # Note: This assumes publications might not have themes yet
            # In practice, arxiv_pipeline already assigns themes from categories
            logger.info("‚ÑπÔ∏è  Les th√®mes ont d√©j√† √©t√© assign√©s par le pipeline arXiv")
            logger.info("‚ÑπÔ∏è  La classification ML additionnelle sera impl√©ment√©e ult√©rieurement")

            # Could add additional ML classification here if needed
            # For example, to refine or add more granular themes

            logger.info("‚úÖ √âtape de classification compl√©t√©e")

    def _update_stats_from_pipeline(self, query: str, stats: ArxivPipelineStats):
        """Mettre √† jour les statistiques globales depuis les stats du pipeline."""
        self.stats['total_collected'] += stats.papers_collected
        self.stats['total_created'] += stats.papers_created
        self.stats['total_updated'] += stats.papers_updated
        self.stats['total_skipped'] += stats.papers_skipped
        self.stats['total_authors'] += stats.authors_created
        self.stats['total_themes'] += stats.themes_created
        self.stats['total_errors'] += stats.errors

        # Per-query stats
        self.stats['query_stats'][query] = {
            'collected': stats.papers_collected,
            'created': stats.papers_created,
            'duration': stats.duration_seconds,
        }

    async def _display_final_stats(self):
        """Afficher statistiques finales d√©taill√©es."""
        logger.info("\n" + "=" * 80)
        logger.info("üìä STATISTIQUES FINALES")
        logger.info("=" * 80)

        async with AsyncSessionLocal() as session:
            pub_repo = PublicationRepository(session)
            author_repo = AuteurRepository(session)
            theme_repo = ThemeRepository(session)

            # Count current database state
            try:
                all_pubs = await pub_repo.list(limit=100000)  # Get count
                pub_count = len(all_pubs)

                all_authors = await author_repo.list(limit=100000)
                author_count = len(all_authors)

                all_themes = await theme_repo.list(limit=100000)
                theme_count = len(all_themes)
            except Exception as e:
                logger.error(f"Erreur lors du comptage : {e}")
                pub_count = "N/A"
                author_count = "N/A"
                theme_count = "N/A"

            logger.info("")
            logger.info("üìö DONN√âES DANS LA BASE :")
            logger.info(f"    Publications    : {pub_count}")
            logger.info(f"    Auteurs         : {author_count}")
            logger.info(f"    Th√®mes          : {theme_count}")
            logger.info("")
            logger.info("üìä OP√âRATIONS EFFECTU√âES :")
            logger.info(f"    Collect√©es      : {self.stats['total_collected']}")
            logger.info(f"    Cr√©√©es          : {self.stats['total_created']}")
            logger.info(f"    Mises √† jour    : {self.stats['total_updated']}")
            logger.info(f"    Ignor√©es        : {self.stats['total_skipped']}")
            logger.info(f"    Nouveaux auteurs: {self.stats['total_authors']}")
            logger.info(f"    Nouveaux th√®mes : {self.stats['total_themes']}")
            logger.info("")

            if self.stats['total_errors'] > 0:
                logger.warning(f"‚ö†Ô∏è  ERREURS : {self.stats['total_errors']}")
            else:
                logger.info("‚úÖ Aucune erreur")

            logger.info("")
            logger.info("üéØ STATISTIQUES PAR REQU√äTE :")
            for query, stats in self.stats['query_stats'].items():
                logger.info(f"    '{query}':")
                logger.info(f"      - Collect√©es : {stats['collected']}")
                logger.info(f"      - Cr√©√©es     : {stats['created']}")
                logger.info(f"      - Dur√©e      : {stats['duration']:.1f}s")


async def main():
    """Point d'entr√©e principal."""
    parser = argparse.ArgumentParser(
        description='Peupler STAGING avec donn√©es r√©elles arXiv',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples:
    # Collecte standard (15000 publications)
    python scripts/populate_real_data.py

    # Collecte limit√©e pour test
    python scripts/populate_real_data.py --max-publications 500

    # Collecte avec batch size personnalis√©
    python scripts/populate_real_data.py --max-publications 10000 --batch-size 50
        """
    )
    parser.add_argument(
        '--max-publications',
        type=int,
        default=15000,
        help='Nombre maximum de publications √† collecter (d√©faut: 15000)'
    )
    parser.add_argument(
        '--batch-size',
        type=int,
        default=100,
        help='Taille des batchs pour les requ√™tes arXiv (d√©faut: 100)'
    )
    parser.add_argument(
        '--date-range-months',
        type=int,
        default=24,
        help='Nombre de mois en arri√®re pour la collecte (d√©faut: 24)'
    )

    args = parser.parse_args()

    populator = RealDataPopulator(
        max_publications=args.max_publications,
        batch_size=args.batch_size,
        date_range_months=args.date_range_months,
    )

    await populator.run()


if __name__ == '__main__':
    asyncio.run(main())
